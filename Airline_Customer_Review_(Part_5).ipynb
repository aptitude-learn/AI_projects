{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [
        "Mk8T-I18kEOD",
        "P56D7jcxlT85",
        "sgfzk1-Tm7sm",
        "4MMuj8VjxOrp",
        "8RQJDxKyyPPP",
        "-tKMxiJ9jV5g",
        "dGkAudff03um",
        "ZL8bMVwb068D",
        "QdFSnFdq1h0j",
        "BzKOAKjx58X2"
      ],
      "authorship_tag": "ABX9TyPuV/dlM0m2BqTDdtdnmiE6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aptitude-learn/AI_projects/blob/main/Airline_Customer_Review_(Part_5).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction"
      ],
      "metadata": {
        "id": "NUowuiiWjZ9H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, we will focus on getting to know our data. This is important for two key reasons:\n",
        "\n",
        "1. Looking closely at our data helps us find patterns, spot missing information, and check if some parts of the data are uneven. Fixing these issues is important because it ensures we get accurate and useful results. By using good quality data, we can give Delta Airlines helpful insights that they can use to make better decisions.\n",
        "\n",
        "2. When sharing your results, it’s important to explain where your data comes from. Giving a clear picture of the data you used helps others trust your findings and understand them better."
      ],
      "metadata": {
        "id": "j-AUGuUojmDE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### So far, you've completed"
      ],
      "metadata": {
        "id": "6UVVT5JOquN4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Data Loading\n",
        "\n"
      ],
      "metadata": {
        "id": "Mk8T-I18kEOD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using a library called pandas, we will load the dataset from a public link hosted on GitHub."
      ],
      "metadata": {
        "id": "bvTeIJ_zkOeV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ZThWAfoV0xA"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('https://gist.githubusercontent.com/almagashi/e8d9e1539069115e00a5a7246fc5cb54/raw/00156b710c9659c59470e77755f26d97e64425f1/airline_data.csv')"
      ],
      "metadata": {
        "id": "UgSyoPw6Wl2J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that our data is loaded, we will take a look at our dataset. Pandas function `.head()` helps us view the first few columns and rows in our data."
      ],
      "metadata": {
        "id": "q37wO9aykasp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "WFAA62E5XQNU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To understand better what columns we have data for, let's print out only the column names."
      ],
      "metadata": {
        "id": "nTDDn-LslpzH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.columns"
      ],
      "metadata": {
        "id": "ImMLFh77ZKfe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we know the columns we're working with, let's dive deeper and take a look at the basic statistics about the values these columns contains, specifically the numerical columns.\n",
        "\n",
        "This step helps us understand the amount and range of data we have, and helps us spot outliers (anomalies) in our data. In this case, we will move forward with the code, as nothing seems out of order."
      ],
      "metadata": {
        "id": "CphuoGF2l0go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.describe()"
      ],
      "metadata": {
        "id": "7MaCd8H-flgR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Data and Duplicate Data"
      ],
      "metadata": {
        "id": "P56D7jcxlT85"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a look at the missing data. Missing data refers to rows or columns that lack any (readable) value.\n",
        "\n",
        "Addressing missing values is a crucial step in the sentiment analysis pipeline, as it can impact the quality of our analysis. We can handle missing data by either removing the incomplete entries or replacing them using a systematic approach, such as imputation."
      ],
      "metadata": {
        "id": "hbIW7gVOkvoF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# missing data counts\n",
        "print(\"Percentage null or na values in our data:\")\n",
        "((data.isnull() | data.isna()).sum() * 100 / data.index.size).round(2)"
      ],
      "metadata": {
        "id": "Uw0kHgEqgWyn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the amount of missing values, and the information the columns contain, we will choose to remove a few irrelevant or useless columns."
      ],
      "metadata": {
        "id": "9HAiuZ7zme_J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# dropping irrelevant columns\n",
        "\n",
        "data = data.drop(['_unit_id', '_golden', '_unit_state', '_trusted_judgments',\n",
        "       '_last_judgment_at', 'airline_sentiment_gold', 'name', 'retweet_count',\n",
        "          'negativereason_gold','tweet_coord',\n",
        "          'tweet_location','user_timezone'], axis=1)"
      ],
      "metadata": {
        "id": "7-IBPAI5ZG5D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "uEpXy4-7mCpf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sometimes, data has duplicate values, due to errors in collection or human error (i.e. submitting the same review twice). These errors should not be reflected in our final analysis, therefore we will check if duplicates exist and remove them."
      ],
      "metadata": {
        "id": "Iw7ndsFYnqjr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# check duplicate values\n",
        "\n",
        "print(f\"Number of duplicate rows: {data.duplicated().sum()}\")"
      ],
      "metadata": {
        "id": "7rGMzl5qnlRt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# drop duplicate values\n",
        "\n",
        "data = data.drop_duplicates()\n",
        "print(f\"Number of duplicate rows after dropping duplicates: {data.duplicated().sum()}\")"
      ],
      "metadata": {
        "id": "WIZ_-u2XnHVf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looks like there are no duplicate values! Let's move to distributions."
      ],
      "metadata": {
        "id": "BmoEKbvSoit2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Data Distributions"
      ],
      "metadata": {
        "id": "sgfzk1-Tm7sm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After examining the data for outliers and missing values, we move to understanding how our data disperses.\n",
        "\n",
        "For example, let's explore how many airlines we are analyzing, how many reviews we have for each airline, and how many of those reviews are negative. This will be important information when presenting your results as your resuls will only be impactful when presented contextually."
      ],
      "metadata": {
        "id": "_xLuRwllnCAO"
      }
    },
    {
      "source": [
        "# count airline data\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "data.groupby('airline').size().plot(kind='barh', color=sns.palettes.mpl_palette('Dark2'))\n",
        "plt.gca().spines[['top', 'right',]].set_visible(False)"
      ],
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "id": "ArNj3ALUaAbo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# plot sentiment distribution\n",
        "\n",
        "data['airline_sentiment'].value_counts().plot(kind='bar', color=sns.palettes.mpl_palette('Dark2'))\n",
        "plt.gca().spines[['top', 'right',]].set_visible(False)"
      ],
      "metadata": {
        "id": "hTcMcfQOhNwA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# show sentiment values per airline\n",
        "\n",
        "a = data.groupby(['airline', 'airline_sentiment'])['airline_sentiment'].count().unstack().plot(kind='bar', stacked=False, color=sns.palettes.mpl_palette('Dark2'))"
      ],
      "metadata": {
        "id": "zZ3njwrEik2j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# show the distributions of negativereason\n",
        "\n",
        "data['negativereason'].value_counts().plot(kind='bar', color=sns.palettes.mpl_palette('Dark2'))\n",
        "plt.gca().spines[['top', 'right',]].set_visible(False)"
      ],
      "metadata": {
        "id": "XIdJbF7jim0x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's see what are the top 3 reasons for each airline's complaints."
      ],
      "metadata": {
        "id": "HuBvoGlmuo2q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# group the data by airline and negativereason, then count the values\n",
        "airline_negativereason_counts = data.groupby(['airline', 'negativereason'])['negativereason'].count().unstack()\n",
        "\n",
        "# for each airline, get the top 3 negative reasons\n",
        "for airline in airline_negativereason_counts.index:\n",
        "  top_3_reasons = airline_negativereason_counts.loc[airline].nlargest(3)\n",
        "\n",
        "  # create a bar plot for the top 3 reasons for the current airline\n",
        "  top_3_reasons.plot(kind='bar', color=sns.palettes.mpl_palette('Dark2'))\n",
        "  plt.title(f'Top 3 Negative Reasons for {airline}')\n",
        "  plt.xlabel('Negative Reason')\n",
        "  plt.ylabel('Count')\n",
        "  plt.gca().spines[['top', 'right',]].set_visible(False)\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "L1HIDaZmtvvS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Data Cleaning"
      ],
      "metadata": {
        "id": "4MMuj8VjxOrp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will apply a few standard steps here for cleaning tweets and extracting most meaning out of it:\n",
        "* remove all links\n",
        "* keep only letters, no emojis\n",
        "* convert all letters to lowercase, and split sentences into words (tokens)\n",
        "* define a set of common English stopwords like: the, at, is, etc.\n",
        "* only keep words that are not in the English stopwords set, because these are where we can extract most meaning from\n",
        "* store the cleaned data into a new column called cleaned_tweet"
      ],
      "metadata": {
        "id": "3L6zal1jnOlF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import and download necessary libraries\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# clean tweets\n",
        "\n",
        "def tweet_to_words(tweet):\n",
        "    nolinks = re.sub(r\"http\\S+\", \"\", tweet)\n",
        "    letters_only = re.sub(\"[^a-zA-Z]\", \" \",nolinks)\n",
        "    words = letters_only.lower().split()\n",
        "    stops = set(stopwords.words(\"english\"))\n",
        "    meaningful_words = [w for w in words if not w in stops]\n",
        "    return( \" \".join( meaningful_words ))"
      ],
      "metadata": {
        "id": "WyRwfsRHxT9j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# store clean tweets into new column called clean_tweet\n",
        "\n",
        "data['clean_tweet']=data['text'].apply(lambda x: tweet_to_words(x))"
      ],
      "metadata": {
        "id": "2Kdqr1WNkf0s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's peek at the data. We see that a new column has been added at the end with the clean text. This is the column we will use in our model."
      ],
      "metadata": {
        "id": "TpmUJydVoVRi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "AFAMm4FOlJaj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's observe how a clean tweet looks against its original counterpart."
      ],
      "metadata": {
        "id": "RRK1Vp6tnNcw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('original tweet:', data.text[75])\n",
        "print('cleaned tweet:', data.clean_tweet[75])"
      ],
      "metadata": {
        "id": "FRDhIS78lSpf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that the most important information has been preserved. We're ready to convert this data into numerical values."
      ],
      "metadata": {
        "id": "co_2rc_FqJAX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Data Transformation"
      ],
      "metadata": {
        "id": "8RQJDxKyyPPP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When we look back at the sentiment distribution, we see most of our data belongs to negative category. he class imbalance might create issues when training our models. Since the task is mainly to classify the negative review, combining neutral and positive into 1 category might help in model training.\n",
        "\n",
        "This is why we will convert all neutral and positive sentiment values into 1, representing positive sentiment and all negative sentiment values to 0."
      ],
      "metadata": {
        "id": "B476Z-uasOTj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data['sentiment_numeric'] = data['airline_sentiment'].map({'negative': 0, 'neutral': 1, 'positive': 1})"
      ],
      "metadata": {
        "id": "ygOJ5Xc5kdPU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "RAQX04LbyRoS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Data Vectorization"
      ],
      "metadata": {
        "id": "-tKMxiJ9jV5g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To get ready for our machine learning algorithm, we need to split the data into two parts: training data and testing data.\n",
        "\n",
        "* The training data is used to teach the algorithm by showing it many examples with labels. For example, we tell the algorithm that \"awful experience\" is negative and \"super experience\" is positive.\n",
        "\n",
        "* The test data is used to see how well the algorithm has learned. We give it new examples, like \"mindblowing experience,\" and see if it predicts the correct label. If the algorithm guesses wrong (like predicting negative when the correct label is positive), this mistake is counted. The percentage of wrong guesses tells us how well the algorithm is performing. We will use these sets to evaluate how our algorithm is performing.\n",
        "\n",
        "The reason we will vectorize these sets separately is to avoid creating any connections between training and test data, so that test data is treated as new unseen data.\n",
        "\n",
        "After all, we want the algorithm in the future to predict any tweet into the correct labels."
      ],
      "metadata": {
        "id": "TITyWJuluCzj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = data['clean_tweet'] # the data we will feed (input)\n",
        "y = data['sentiment_numeric'] # the labels it will learn against (output)"
      ],
      "metadata": {
        "id": "Tz1RsL0OuCFC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# split the data into 80% training, 20% testing randomly\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "ehE8D3p4wDq0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will vectorize the train and test data separately."
      ],
      "metadata": {
        "id": "quhmRY_vwkWT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf = TfidfVectorizer()\n",
        "tfidf.fit(X_train)"
      ],
      "metadata": {
        "id": "T1bSmgY0jXja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_tfidf = tfidf.transform(X_train)\n",
        "X_test_tfidf = tfidf.transform(X_test)"
      ],
      "metadata": {
        "id": "IZ1PtxUkwbF6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After the vectorizer turns the data into vectors, you should have your data look like a matrix. Let's see if that's the format we have our data on right now:"
      ],
      "metadata": {
        "id": "-ZIroSu9w6GQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_tfidf"
      ],
      "metadata": {
        "id": "VxyeFVpOwy8h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_tfidf"
      ],
      "metadata": {
        "id": "sSPIuqY1w1cS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Choosing models"
      ],
      "metadata": {
        "id": "dGkAudff03um"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import the chosen models\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.naive_bayes import MultinomialNB"
      ],
      "metadata": {
        "id": "o1n6ifobyo1F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training the models"
      ],
      "metadata": {
        "id": "ZL8bMVwb068D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# activate and train Logistic Regression Model\n",
        "\n",
        "log = LogisticRegression(max_iter=1000)\n",
        "log.fit(X_train_tfidf,y_train)"
      ],
      "metadata": {
        "id": "4plYAL6Y0v4Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We know the logistic model has ran when the output shows `LogisticRegression(max_iter=1000)`."
      ],
      "metadata": {
        "id": "VqT0vd1-wJnD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# activate and train Linear Support Vector Machine Model\n",
        "\n",
        "svc = LinearSVC()\n",
        "svc.fit(X_train_tfidf,y_train)"
      ],
      "metadata": {
        "id": "ZtTR3t4Ixymv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# activate and train Multinomial Naive Bayes Model\n",
        "\n",
        "nb = MultinomialNB()\n",
        "nb.fit(X_train_tfidf,y_train)"
      ],
      "metadata": {
        "id": "_w__M8aZ0x9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that all models have ran and trained successfully, let's move to testing and evaluating their performance."
      ],
      "metadata": {
        "id": "7z6oJK6nwXyF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "gPm7G0gvBQD2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Testing the models"
      ],
      "metadata": {
        "id": "QdFSnFdq1h0j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preds_log = log.predict(X_test_tfidf)\n",
        "preds_svc = svc.predict(X_test_tfidf)\n",
        "preds_nb = nb.predict(X_test_tfidf)"
      ],
      "metadata": {
        "id": "Ln-lznpBxJxW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Metrics of Evaluation"
      ],
      "metadata": {
        "id": "Al5lod1p1d4O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's begin with the most common, accuracy.\n",
        "\n",
        "**Accuracy**\n",
        "\n",
        "As the minimum benchmark, a model is doing good, when it classifies better than chance (chance is when accuracy is at 50%, because if we were to randomly guess whether a review is positive or negative, we will likely get it right about 50% of the time). As such accuracy being above 50% is the minimum requirement.\n",
        "\n",
        "However, depending on the problem at hand, we need to consider a few more metrics. If the classifier will be tasked at classifyigng whether patients have cancer or not, we sure need much higher accuracy, on top of, making sure the classifier doesn't make critical mistakes. This brings us to two important types of errors: Type I and Type II errors, which are especially important in problems like diagnosing cancer.\n",
        "\n",
        "* **Type I Error** (False Positive): This occurs when the model predicts something as positive when it’s actually negative. For example, predicting a patient has cancer when they do not.\n",
        "* **Type II Error** (False Negative): This happens when the model predicts something as negative when it’s actually positive. For instance, predicting a patient does not have cancer when they actually do.\n",
        "\n",
        "To measure how well our model avoids these mistakes, we use a few key metrics:\n",
        "\n",
        "1. **Precision**: Precision helps us understand how often the model’s positive predictions are correct. It is the ratio of true positives (correct positive predictions) to all positive predictions (true positives + false positives). If we care more about avoiding Type I errors (false positives), we want high precision. For example, in spam detection, we want to avoid marking important emails as spam (false positives).\n",
        "\n",
        "2. **Recall**: Recall tells us how well the model captures all actual positive cases. It is the ratio of true positives to all actual positives (true positives + false negatives). In cases like cancer detection, avoiding Type II errors (false negatives) is critical, so we focus on maximizing recall, ensuring we detect as many positive cases as possible.\n",
        "\n",
        "\n",
        "3. **F1 Score**: The F1 score is a balance between precision and recall, providing a single measure of the model’s performance when both false positives and false negatives matter. It’s the harmonic mean of precision and recall, giving a better sense of the model’s effectiveness when we need to balance both types of errors.\n",
        "\n",
        "\n",
        "By using these metrics alongside accuracy, we ensure that our model not only makes accurate predictions but also minimizes the most harmful errors for the task at hand.\n",
        "\n",
        "When deciding how to improve a model, we choose the metric that best fits the problem. In our case, we need to make sure most negative reviews are flagged, and not be mistakenly labelled as \"positive\". However, we also need to make sure that Delta employers who are working hard do not have to sift through positive reviews in the negative review bunch. So we will choose our model based on F-1 Score."
      ],
      "metadata": {
        "id": "fMF0ZVa-1b3s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# create a report of all metrics of classification for logisitic regression\n",
        "\n",
        "metrics_log = classification_report(y_test,preds_log)\n",
        "\n",
        "# print all metric results\n",
        "\n",
        "print(metrics_log)"
      ],
      "metadata": {
        "id": "uuuYskI-12xL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a report of all metrics of classification for support vector machine\n",
        "\n",
        "metrics_svc = classification_report(y_test,preds_svc)\n",
        "\n",
        "# print all metric results\n",
        "\n",
        "print(metrics_svc)"
      ],
      "metadata": {
        "id": "kfM6SjTX0QLG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a report of all metrics of classification for multinomial naive bayes\n",
        "\n",
        "metrics_nb = classification_report(y_test,preds_nb)\n",
        "\n",
        "# print all metric results\n",
        "\n",
        "print(metrics_nb)"
      ],
      "metadata": {
        "id": "GE1V94fq2Ncc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "These are the scores we are interested in:\n",
        "\n",
        "\n",
        "| Model                     | Accuracy | F1-Score|\n",
        "|----------------------------|----------|---------------------------|\n",
        "| Logistic Regression         | 83%      | 83%                       |\n",
        "| Support Vector Machine      | 83%      | 83%                       |\n",
        "| Multinomial Naive Bayes     | 80%      | 79%                       |\n",
        "\n",
        "\n",
        "All of our models have an accuracy score that meets the minimum standard.\n",
        "\n",
        "While Logistic Regression and Support Vector Machines show slightly higher accuracy than Multinomial Naive Bayes, the F1-Score tells a more balanced story. The F1-Score balances precision and recall, making it a better measure when both false positives and false negatives matter.\n",
        "\n",
        "In this case, the F1-Score for Multinomial Naive Bayes is slightly lower than for the other models. This means it struggles more in correctly classifying both positive and negative reviews, compared to the other models, which perform slightly better overall.\n",
        "\n",
        "However, since the F1-Score difference is small, the choice between models depends on priorities. If Delta prefers catching all negative reviews and is okay with a few false positives, they might still consider Multinomial Naive Bayes. But based on the balanced performance of both precision and recall, Logistic Regression or Support Vector Machines would be stronger choices for more consistent overall performance."
      ],
      "metadata": {
        "id": "kQQR5Zlw4Lji"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Finally...\n",
        "\n",
        "The whole purpose of this classifier is to work beyond the dataset we worked with. For example, any new reviews that come through, this classifier should be able to guess correctly about 80% of the time if they're negative, and flag them to Delta's customer service team.\n",
        "\n",
        "So let's test and see if our model will work with any reviews:"
      ],
      "metadata": {
        "id": "BzKOAKjx58X2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# create a pipeline to bring together the vectorizer and classifier and train it\n",
        "\n",
        "pipe = Pipeline([('tfidf',TfidfVectorizer()),('Logistic Regression', LogisticRegression())])\n",
        "pipe.fit(data['clean_tweet'], data['sentiment_numeric'])"
      ],
      "metadata": {
        "id": "yFW9Z3mC6fpT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import array\n",
        "\n",
        "# convert our score from 0 and 1 to negative or non-negative, respectively\n",
        "\n",
        "def predict(new_tweet):\n",
        "  prediction = pipe.predict(new_tweet)\n",
        "  if prediction == array([0]):\n",
        "    print('Negative')\n",
        "  else:\n",
        "    print('Non-negative')"
      ],
      "metadata": {
        "id": "r_daCG_Q7i-I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test with real review taken from TripAdvisor\n",
        "\n",
        "new_tweet = ['we had a terrible experience with delta.having a very long flight from Cancun to Warsaw with 2 stops, we bought tickets quite in advance and paid extra to select seats...']\n",
        "predict(new_tweet)"
      ],
      "metadata": {
        "id": "fjoFuaeC80Nz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test with real review taken from TripAdvisor\n",
        "\n",
        "new_tweet = ['Wow, what great service, pricing and travel experience my husband, and I just had, 8-23-24...']\n",
        "predict(new_tweet)"
      ],
      "metadata": {
        "id": "ikwpzSsV9O05"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And that concludes this project. You've successfully gone through all the steps below:\n",
        "1. Data Loading\n",
        "2. Missing Data and Duplicate Data\n",
        "3. Data Distributions\n",
        "4. Data Cleaning\n",
        "5. Data Transformation\n",
        "6. Data Vectorization\n",
        "7. Choosing models\n",
        "8. Training the models\n",
        "9. Testing the models\n",
        "10. Metrics of evaluation\n",
        "\n",
        "and finally:\n",
        "\n",
        "11. Testing your model with unseen data"
      ],
      "metadata": {
        "id": "Q4N400iECcNt"
      }
    }
  ]
}